{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCwMDZp6NpjG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec, Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUCT6afBIUUc"
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6AYmpmLITkZ"
   },
   "outputs": [],
   "source": [
    "!pip install cupy-cuda101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgAOdLhyNsEG"
   },
   "outputs": [],
   "source": [
    "priorart_PATH = \"/content/gdrive/My Drive/GQP/Data/df_priorart.csv\"\n",
    "priorart = pd.read_csv(priorart_PATH)\n",
    "priorart['noun'] = data['noun'].astype('str')\n",
    "appeal_PATH = \"/content/gdrive/My Drive/GQP/Data/df_appeal.csv\"\n",
    "appeal = pd.read_csv(appeal_PATH)\n",
    "appeal['noun'] = data['noun'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3AWnwyC1Dhc"
   },
   "outputs": [],
   "source": [
    "df_appeal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpvRAtrK1JBz"
   },
   "outputs": [],
   "source": [
    "Reversed = df_appeal[df_appeal['decision']=='Reversed']\n",
    "Affirmed = df_appeal[df_appeal['decision']=='Affirmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEQnbW7P1UhE"
   },
   "outputs": [],
   "source": [
    "Reversed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_uDYgzqmyar"
   },
   "source": [
    "# draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca2VBibYmu0t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agTOikHEEqPk"
   },
   "outputs": [],
   "source": [
    "wordDict = set()\n",
    "data['length'] = 0\n",
    "for i in range(data.shape[0]):\n",
    "  if i % 100000 == 0:\n",
    "    print(i)\n",
    "  data['length'][i]= len(data['noun'][i])\n",
    "  words = data['noun'][i].split(' ')\n",
    "  for w in words:\n",
    "    wordDict.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7Oz7OW5F0jy"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1-JWqjyF0DF"
   },
   "outputs": [],
   "source": [
    "data[data.length < 10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a97Pmn02Oefi"
   },
   "outputs": [],
   "source": [
    "data = data[data['length'] >= 10] \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEe7prkDPl5A"
   },
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords): \n",
    "    word_count = dict.fromkeys(wordDict, 0)\n",
    "    for word in bagOfWords:\n",
    "      word_count[word] += 1\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in word_count.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ue-04uJ0eqqJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5X8BJSkZX_Eo"
   },
   "outputs": [],
   "source": [
    "len(data.index), data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "St6lDi3EUjmX"
   },
   "outputs": [],
   "source": [
    "# path = \"/content/gdrive/My Drive/\"\n",
    "# file = open(path+'words.csv', 'w')\n",
    "# file.write(', '.join(wordDict)+'\\n')\n",
    "\n",
    "for i in tqdm(data.index):  \n",
    "  di = data['noun'][i].split(' ')\n",
    "  di_tf = computeTF(wordDict, di)\n",
    "  line = []\n",
    "  for k in di_tf:\n",
    "    line.append(str(di_tf[k]))\n",
    "  break\n",
    "  # file.write(','.join(line)+'\\n')  \n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAf-CzS1ZCZF"
   },
   "outputs": [],
   "source": [
    "arr = np.array(list(di_tf.values()))\n",
    "arr[arr!=0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M1I6lJOi60J"
   },
   "outputs": [],
   "source": [
    "a = np.zeros(10,10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pv-jKfHiiq9C"
   },
   "outputs": [],
   "source": [
    "TF = np.zeros((data.shape[0], len(wordDict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DedSnteXYck1"
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "for k in di_tf:\n",
    "  a.append(di_tf[k])\n",
    "s = ','.join([str(n) for n in a])\n",
    "print(len(s.split(',')))\n",
    "print(len([float(n) for n in s.split(',') if float(n)>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1rVDWJ1Poi8"
   },
   "outputs": [],
   "source": [
    "def computeIDF(documents):    \n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZXUXZ6SlaNv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xPtCFbQla4f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YA_XeW1dlaq8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdWL62IbqP_s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVEL-6iWmfsb"
   },
   "source": [
    "# TF-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qsO0JT4TG6N"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(data['noun'])\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQFbj2_wlNM0"
   },
   "outputs": [],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uUyVTzPlzRo"
   },
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9HVD6Tf4033"
   },
   "outputs": [],
   "source": [
    "vectorizer_appeal = TfidfVectorizer()\n",
    "vectors_appeal = vectorizer_appeal.fit_transform(df_appeal['noun'])\n",
    "feature_names_appeal = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0BqJ9qK5N5h"
   },
   "outputs": [],
   "source": [
    "vectors_appeal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v86cuXd9mnY4"
   },
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxJe1CfUqyIi"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i in tqdm(range(priorart.shape[0])):\n",
    "  sentences.append(priorart['noun'][i].split(' '))\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmDJflFLmpeg"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "model = Word2Vec(min_count = 1,  size = 100, window = 5, sg= 1, workers = cores)\n",
    "# model1 = gensim.models.Word2Vec(data['noun'], ) \n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EevqqRtynlQs"
   },
   "outputs": [],
   "source": [
    "Vocab = model.wv.vocab\n",
    "len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD3_GmtQsbOT"
   },
   "outputs": [],
   "source": [
    "# from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vak56Y2kslz7"
   },
   "outputs": [],
   "source": [
    "model.train(sentences, total_examples=model.corpus_count, epochs=20, compute_loss=True, callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PY6zp2GGss4c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgYuw_En0i95"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdFAT_jg0okl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKn1Cj-_0pC9"
   },
   "source": [
    "# Cosine similarty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HGyMyfN7UGd"
   },
   "outputs": [],
   "source": [
    "new_word = []\n",
    "for w in tqdm(feature_names_appeal):\n",
    "  if w not in feature_names:\n",
    "    new_word.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naiK4U-A4q9S"
   },
   "outputs": [],
   "source": [
    "vectors_appeal[Reversed.index[0]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdv3ltAY4rmW"
   },
   "outputs": [],
   "source": [
    "Reversed.shape[0] + Affirmed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fstIw-tP6TuM"
   },
   "outputs": [],
   "source": [
    "priorart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5n0e9WY4sK2"
   },
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiDng9S20ndC"
   },
   "outputs": [],
   "source": [
    "Reversed['new'] = 0\n",
    "# Reversed['new_nouns'] = 0\n",
    "new_nouns = [] #[id, number, list]\n",
    "for i in tqdm(Reversed.index):\n",
    "  sentence = Reversed['noun'][i].split(' ')\n",
    "  count = 0\n",
    "  new = []\n",
    "  for w in sentence:\n",
    "    if w not in feature_names:\n",
    "      count +=1\n",
    "      new.append(w)\n",
    "  if count > 0:\n",
    "    new_nouns.append([i, count, new])\n",
    "  Reversed['new'][i] = count\n",
    "  # Reversed['new_nouns'][i] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bF4rwe31wfw"
   },
   "outputs": [],
   "source": [
    "Reversed[Reversed.new>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idLnZerV2C_L"
   },
   "outputs": [],
   "source": [
    "Affirmed['new'] = 0\n",
    "new_nouns_affirmed = [] #[id, number, list]\n",
    "for i in tqdm(Affirmed.index):\n",
    "  sentence = Affirmed['noun'][i].split(' ')\n",
    "  count = 0\n",
    "  new = []\n",
    "  for w in sentence:\n",
    "    if w not in feature_names:\n",
    "      count +=1\n",
    "      new.append(w)\n",
    "  if count > 0:\n",
    "    new_nouns_affirmed.append([i, count, new])\n",
    "  Affirmed['new'][i] = count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zikkRwqS4F77"
   },
   "outputs": [],
   "source": [
    "Affirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkM5a9Xv4pMs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNYFKJm3o7PnadeylMpgCgF",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "cosine.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
